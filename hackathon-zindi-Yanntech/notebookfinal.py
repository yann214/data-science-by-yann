# -*- coding: utf-8 -*-
"""StarterNotebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ucjncho1bDClHa0n_kDv4Q0vEtzYsAhA

# üéâ Welcome to the ' SheCures: AI for Diabetes Prediction ‚Äì Celebrating International Girls in ICT Day' Hackathon Starter Notebook
This notebook will help you get started on your journey to building an AI model for predicting diabetes types in African women.
**Organized by Dare to be Women Tech** in celebration of *International Girls in ICT Day*.
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""## üß© Problem Statement
Your task is to build a machine learning model that can classify the type of diabetes (e.g., Type 2 or gestational) in women based on health-related features.
"""

# !pip install -U scikit-learn

# üì¶ Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score, classification_report

import warnings
warnings.filterwarnings("ignore")

# üìÇ Load the dataset (update the path as needed)
df = pd.read_csv("data/Train.csv")
df

# ‚ÑπÔ∏è Dataset information
df.info()

# üìä Basic statistics
df.describe()

# Class distribution
df['Target'].value_counts()

## encodage de la variable cible pour le model xgboost
from sklearn.preprocessing import LabelEncoder

y = df.Target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Voir toutes les classes dans l'ordre
class_name = le.classes_
for i, class_name in enumerate(le.classes_):
    print(f"{class_name} ‚Üí {i}")

#  Correlation heatmap Pour la visualisation de la correlation entre les variables.
sns.heatmap(df.corr(numeric_only = True),annot = True)



"""# **separation** des donnees"""

X = df.drop(["Target", "ID"], axis=1)
Y = df.Target
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_features

# üéØ Data preprocessing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

num_features = numerical_features
cat_features = categorical_features

num_transformer = Pipeline([
    ("imputer", KNNImputer(n_neighbors = 5)),
    ("scaler", StandardScaler()) ## normalisation des donnees numerique avec StandardScaler
])

# cat_transformer = Pipeline([
#     ("encoder", OneHotEncoder(handle_unknown = "ignore"))
# ])
cat_transformer = Pipeline([
    ("encoder", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) #utilisation du label encoding pour les variables categorielles
])

preprocessor = ColumnTransformer([
    ("num", num_transformer, num_features),
    ("cat", cat_transformer, cat_features)
])

# Encode categorical variables and split features/target as needed
df_cleaned = preprocessor.fit_transform(X)
new_columns = (
    num_features +
    list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))
)

# Convert in Pandas DataFrame
df_cleaned = pd.DataFrame(df_cleaned, columns=new_columns)

X_train,X_test,y_train,y_test = train_test_split(df_cleaned, y_encoded, test_size = 0.2, random_state = 42)

# ü§ñ Baseline prediction Model
# Mod√®le SVM
svm_model = SVC(kernel="rbf", C=1.0, gamma="scale", probability=True)

# For√™t al√©atoire
forest_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)

# Arbre de d√©cision
tree_model = DecisionTreeClassifier(random_state=42)

# R√©gression logistique
reg_model = LogisticRegression(max_iter=1000, solver="lbfgs", multi_class="auto")

models =[svm_model, forest_model, tree_model, reg_model]

def train_model(model,xtrain,ytrain,xtest,ytest):
    print("", model)
    model.fit(xtrain,ytrain)

    ypred = model.predict(xtest)
    report = classification_report(ypred, ytest)
    acc = accuracy_score(ypred, ytest)

    return ypred, acc


for m in models:
    ypred , acc = train_model(m, X_train, y_train, X_test, y_test)
    print(acc)

#choose the best model
best_model = forest_model
y_pred = best_model.predict(X_test)
report = classification_report(y_pred, y_test)
print(report)

best_model.score(X_test, y_test)

## creation du model avec les meilleurs parametres
best_model = RandomForestClassifier(n_estimators=200, min_samples_leaf = 1, min_samples_split=2, random_state=42)

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
report = classification_report(y_pred, y_test)
print(report)
best_model.score(X_test, y_test)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Pr√©dictions
y_pred = best_model.predict(X_test)

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap='Blues')

from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(best_model, df_cleaned, y_encoded, cv=cv, scoring='f1_macro')
print("F1 Macro Score:", scores.mean())

"""#Phase Test"""

# üì§ Generate submission file (update for actual test set)
test_df = pd.read_csv("data/Test.csv")
test_X = test_df.drop(["ID"], axis=1)

df_cleaned = preprocessor.transform(test_X)
new_columns = (
    num_features +
    list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))
)
# Convert in Pandas DataFrame
df_cleaned = pd.DataFrame(df_cleaned, columns=new_columns)
df_cleaned

predictions = best_model.predict(df_cleaned)
predictions

submission3 = pd.DataFrame({"id": test_df['ID'], "diabetes_type": predictions})
submission3

submission3.to_csv('submission3.csv', index=False)

"""## üöÄ Next Steps
- Try more advanced models (XGBoost, etc.)
- Perform feature engineering and selection
- Consider model interpretability (SHAP, LIME) for healthcare applications

Good luck, and thank you for being part of this impactful challenge! üíô
"""

# !pip install xgboost

import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# ‚ö†Ô∏è Assure-toi que tes donn√©es sont d√©j√† nettoy√©es et transform√©es
# (X_train_cleaned, X_test_cleaned, y_train, y_test)

# Cr√©ation du mod√®le
xgb_model = XGBClassifier(
    objective="multi:softmax",  # ou "multi:softprob" pour probabilit√©s
    num_class=4,                # nombre de classes dans la target
    eval_metric="mlogloss",
    use_label_encoder=False,
    random_state=42
)

# Entra√Ænement
xgb_model.fit(X_train, y_train)

# Pr√©dictions
y_pred_xgb = xgb_model.predict(X_test)

# √âvaluation
print("Accuracy :", accuracy_score(y_test, y_pred_xgb))
print(classification_report(y_test, y_pred_xgb))

"""# Recherche des meilleurs parametres"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')

grid_search = GridSearchCV(estimator=xgb,
                           param_grid=param_grid,
                           scoring='accuracy',
                           cv=5,
                           verbose=1,
                           n_jobs=-1)

grid_search.fit(X_train, y_train)

print("Meilleurs param√®tres :", grid_search.best_params_)
best_xgb = grid_search.best_estimator_

# Cr√©ation du mod√®le avec les meilleurs hyperparam√®tres
best_xgb_model = XGBClassifier(
    colsample_bytree=0.8,
    learning_rate=0.01,
    max_depth=7,
    n_estimators=200,
    subsample=1,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

# Entra√Ænement
best_xgb_model.fit(X_train, y_train)

# Pr√©dictions
y_pred = best_xgb_model.predict(X_test)

# √âvaluation
from sklearn.metrics import accuracy_score, classification_report

print("‚úÖ Accuracy :", accuracy_score(y_test, y_pred))
print("\nüìä Rapport de classification :\n")
print(classification_report(y_test, y_pred))

"""on obtient le meme score que celui de randomForestClassifier"""

from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(best_xgb_model, df_cleaned, y_encoded, cv=cv, scoring='f1_macro')
print("F1 Macro Score:", scores.mean())

"""<p>Avec une cross validation de  0.9824357521531063 l√©g√®rement superieur a celle de randomForestClassifier. cela nous permet de d'affirmer que notre model generalise bien et est plus robuste que RandomForestClassifier. </p>

## visualisation de la matrice de confusion avec le model xgboost
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Pr√©dictions
y_pred = xgb_model.predict(X_test)

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_model.classes_)
disp.plot(cmap='Blues')

"""## Phase de Test"""

# üì§ Generate submission file (update for actual test set)
test_df = pd.read_csv('data/Test.csv')
test_X = test_df.drop(["ID"], axis=1)

df_cleaned_test = preprocessor.transform(test_X)
new_columns = (
    num_features +
    list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))
)
# Convert in Pandas DataFrame
df_cleaned_test = pd.DataFrame(df_cleaned_test, columns=new_columns)
df_cleaned_test

predictions = xgb_model.predict(df_cleaned)
predictions

y_class = le.classes_
print(y_class)

predictions_decoded = pd.Series(predictions).replace({0: "Gestational Diabetes", 1: "Prediabetic", 2: "Type 1 Diabetes", 3: "Type 2 Diabetes"})
predictions_decoded

submission3 = pd.DataFrame({"id": test_df['ID'], "diabetes_type": predictions_decoded})
submission3

submission3.to_csv('submission3.csv', index=False)

"""## visualisation des variables importantes pour les predictions"""

#Affichage de l'importance des variables
importance = pd.Series(best_xgb_model.feature_importances_ ,index=X_train.columns)
importance_sorted = importance.sort_values()
plt.barh(importance_sorted.index, importance_sorted)
plt.title('importance des variables dans la prediction')
plt.xlabel('Importance')
plt.ylabel('variables')
plt.figure(figsize=(25, 20))
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import XGBClassifier

# R√©cup√©rer les importances
importances = best_xgb_model.feature_importances_
features = X_train.columns
importance_df = pd.DataFrame({
    'feature': features,
    'importance': importances
}).sort_values(by="importance", ascending=False)

# Seuil : 3% d‚Äôimportance : on recupere toutes les variables dont l'importance est inferieur a 3%
low_importance_features = importance_df[importance_df["importance"] < 0.03]["feature"].tolist()
print(f"Variables √† envisager pour suppression ({len(low_importance_features)}):\n", low_importance_features)

"""## Entrainement du model avec les features les plus importantes"""

X_train_reduced = X_train.drop(columns=low_importance_features)
X_test_reduced = X_test.drop(columns=low_importance_features)

model_reduced = XGBClassifier(
    colsample_bytree=0.8,
    learning_rate=0.01,
    max_depth=7,
    n_estimators=200,
    subsample=1,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
model_reduced.fit(X_train_reduced, y_train)

accuracy_reduced = model_reduced.score(X_test_reduced, y_test)
print("Accuracy apr√®s suppression des variables peu importantes :", accuracy_reduced)

"""le score diminue"""

# Pr√©dictions
y_pred = best_xgb_model.predict(X_test)

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_xgb_model.classes_)
disp.plot(cmap='Blues')

import shap

# 1. SHAP explainer
explainer = shap.Explainer(best_xgb_model, X_train)

# 2. SHAP values (pour chaque pr√©diction et chaque feature)
shap_values = explainer(X_train)

# 3. Moyenne des impacts
shap.summary_plot(shap_values, X_train)

"""Les r√©sultats nous montre que les features tels que le BMI, le taux de glucose et le tour de taille, ressortent comme les facteurs les plus d√©terminants, toutes classes confondues.

## Choix  du model final pour la soumission
on va sauvegarder le model bas√© sur le randomforestclassifier
"""

import joblib

joblib.dump(best_model, 'best_model.pkl')

